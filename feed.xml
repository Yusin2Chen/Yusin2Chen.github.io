<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://yusin2chen.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://yusin2chen.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-12T22:29:54+00:00</updated><id>https://yusin2chen.github.io/feed.xml</id><title type="html">BeyondEarth</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">The First Step of Open GeoAgent - A Technical Dive into Finding Useful Geospatial Data Sources</title><link href="https://yusin2chen.github.io/blog/2025/Geoparquet/" rel="alternate" type="text/html" title="The First Step of Open GeoAgent - A Technical Dive into Finding Useful Geospatial Data Sources"/><published>2025-05-27T21:01:00+00:00</published><updated>2025-05-27T21:01:00+00:00</updated><id>https://yusin2chen.github.io/blog/2025/Geoparquet</id><content type="html" xml:base="https://yusin2chen.github.io/blog/2025/Geoparquet/"><![CDATA[<h2 id="abstract">abstract</h2> <p>Efficiently discovering and preparing optimal geospatial datasets from heterogeneous, large-scale open repositories like Google Earth Engine (GEE), Microsoft Planetary Computer, NASA Earth Data, and AWS Open Data is a critical bottleneck for advanced geospatial analysis and AI applications. Open GeoAgent’s initial phase tackles this by implementing a robust system for discovering and preparing relevant geospatial data sources.</p> <p>This post details a two-stage architecture:</p> <ol> <li>A <strong>data processing pipeline</strong> converting STAC (SpatioTemporal Asset Catalog) metadata to GeoParquet, indexed by DuckDB for high-performance spatio-temporal querying.</li> <li>An <strong>intelligent retrieval layer</strong> using a multi-document Retrieval Augmented Generation (RAG) system built with LlamaIndex to identify task-specific collections and extract necessary parameters.</li> <li>scripts and geoparquet data link: https://huggingface.co/datasets/Yusin/GeoAgent-Geoparquet</li> </ol> <h2 id="part-1-high-performance-data-preparation-with-stac-geoparquet-and-duckdb">Part 1: High-Performance Data Preparation with STAC, GeoParquet, and DuckDB</h2> <p>The foundation of our data discovery pipeline is the standardized STAC metadata. To optimize for analytical queries, STAC Items from target collections across major open data providers are transformed into GeoParquet files.</p> <p><strong>1. STAC to GeoParquet Transformation:</strong></p> <ul> <li><strong>Why GeoParquet?</strong> GeoParquet is an open, cloud-optimized, columnar format for geospatial vector data. Its columnar nature is key, allowing efficient storage and partial reads. This structure enables query engines like DuckDB to leverage predicate pushdown (filtering data at the source before reading it into memory) and columnar vectorization for significantly faster I/O and query processing. This is particularly advantageous compared to row-oriented formats or iterating through individual STAC items via an API for large-scale filtering.</li> <li><strong>Organization</strong>: Typically, each set of GeoParquet files corresponds to STAC items from a specific data collection, maintaining a clear and organized data lake structure. The schema of the GeoParquet files is derived directly from the STAC Item structure, including common metadata fields, asset links, and the geometry.</li> </ul> <p><strong>2. DuckDB for Accelerated Indexing and Querying:</strong></p> <p>DuckDB, an in-memory OLAP (Online Analytical Processing) DBMS, is employed for its exceptional speed, ease of integration (especially its Python bindings and direct Parquet reading capabilities), and rich SQL dialect.</p> <p>Key DuckDB features utilized:</p> <ul> <li> <p>Direct Parquet Querying: DuckDB can directly query one or more Parquet files, including those stored in cloud object storage.</p> <p>Python</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import duckdb
conn = duckdb.connect()
conn.sql("SELECT count(id) FROM 'path/to/your/collection_geoparquet/*.parquet';")
</code></pre></div> </div> </li> <li> <p>Spatial Extension: This extension is crucial for geospatial filtering. After installation (INSTALL spatial; LOAD spatial), powerful spatial SQL operations can be performed.</p> <p>Python</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Example Python usage with DuckDB
import duckdb
  
# It's good practice to install and load extensions once per session if needed.
# For persistent storage, these might be set in the database configuration.
conn = duckdb.connect()
conn.execute("INSTALL spatial;")
conn.execute("LOAD spatial;")
  
# Define WKT for an Area of Interest (AOI) and time range
aoi_wkt = "POLYGON((-5.0 47.0, -5.0 48.0, -4.0 48.0, -4.0 47.0, -5.0 47.0))" # Example polygon
start_time = "2023-03-01T00:00:00Z"
end_time = "2023-08-31T23:59:59Z"
  
query = f"""
SELECT id, properties_datetime, assets_B04_href, properties_eo_cloud_cover
FROM read_parquet('path_to_your_geoparquet_files/*.parquet', union_by_name=True)
WHERE "sar:product_type" = 'GRD' -- Example for Sentinel-1
  AND "sar:instrument_mode" = 'IW' -- Example for Sentinel-1
  AND ST_Intersects(geometry, ST_GeomFromText('{aoi_wkt}'))
  AND properties_datetime &gt;= '{start_time}'
  AND properties_datetime &lt;= '{end_time}'
  AND properties_eo_cloud_cover &lt; 20; -- Example cloud cover filter
"""
result = conn.execute(query).fetchdf()
print(result.head())
</code></pre></div> </div> </li> <li> <p>CQL2 to SQL Translation: To maintain compatibility with existing STAC API workflows and allow users to leverage familiar query languages, the pygeofilter library, along with its library, along with its pygeofilter-duckdb backend, can parse CQL2-JSON filters and translate them into DuckDB SQL WHERE clauses. This allows users to define filters once and apply them to both STAC APIs and the GeoParquet/DuckDB backend.</p> <p>Python</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Conceptual Python usage with pygeofilter
from pygeofilter.parsers.cql2_json import parse as json_parse
from pygeofilter.backends.duckdb import to_sql_where
from pygeofilter.util import IdempotentDict
  
cql2_filter = {
  "op": "and",
  "args": [
    {"op": "between", "args": [{"property": "eo:cloud_cover"}, 0, 21]},
    {"op": "between", "args": [{"property": "datetime"}, "2023-02-01T00:00:00Z", "2023-02-28T23:59:59Z"]},
    {"op": "s_intersects", "args": [{"property": "geometry"}, {"type": "Polygon", "coordinates": [[[...]]]}]}
  ]
}
# field_mapping can be used if property names differ from GeoParquet column names
sql_where_clause = to_sql_where(json_parse(cql2_filter), IdempotentDict())
# full_query = f"SELECT * FROM read_parquet('{geoparquet_path}') WHERE {sql_where_clause}"
</code></pre></div> </div> </li> </ul> <p>This spatio-temporal pre-filtering significantly prunes the search space, making the subsequent semantic retrieval step more focused and computationally feasible.</p> <h2 id="part-2-intelligent-collection-selection-via-multi-document-rag-with-llamaindex">Part 2: Intelligent Collection Selection via Multi-Document RAG with LlamaIndex</h2> <p>Even after spatio-temporal filtering, multiple data collections might meet the basic criteria. Selecting the <em>most suitable</em> collection for a nuanced task (e.g., “which LiDAR dataset is best for sub-meter vertical accuracy canopy height models in a temperate forest region?”) requires deeper semantic understanding of each collection’s specifications, processing levels, and suitability for specific applications. A Multi-Document Retrieval Augmented Generation (RAG) system, built using LlamaIndex, provides this capability.</p> <p><strong>1. Architecture Overview:</strong></p> <p>The system employs a two-tiered agent structure:</p> <ul> <li><strong>Collection-Specific Agents (<code class="language-plaintext highlighter-rouge">FunctionAgent</code>)</strong>: Each candidate geospatial data collection, represented by its detailed metadata, user guides, scientific papers describing it, or even API documentation, is managed by a dedicated LlamaIndex <code class="language-plaintext highlighter-rouge">FunctionAgent</code>.</li> <li><strong>Top-Level Orchestrator Agent (<code class="language-plaintext highlighter-rouge">FunctionAgent</code> or <code class="language-plaintext highlighter-rouge">ReActAgent</code>)</strong>: This agent receives the user’s primary query and intelligently routes sub-queries to the appropriate collection-specific agents.</li> </ul> <p><strong>2. Building Collection-Specific Agents:</strong></p> <p>The process, inspired by LlamaIndex’s multi-document agent patterns, involves creating specialized agents for each data collection:</p> <ul> <li> <p><strong>Document Processing</strong>: For each collection, relevant documents (e.g., landing pages, technical specifications, usage tutorials) are loaded and parsed into nodes using <code class="language-plaintext highlighter-rouge">SentenceSplitter</code>.</p> </li> <li> <p>Index Creation:</p> <ul> <li><code class="language-plaintext highlighter-rouge">VectorStoreIndex</code>: Built from these nodes using an embedding model (e.g., <code class="language-plaintext highlighter-rouge">OpenAIEmbedding(model="text-embedding-3-small")</code>). This index allows for semantic search within the collection’s documentation (e.g., “find information about radiometric correction”).</li> <li><code class="language-plaintext highlighter-rouge">SummaryIndex</code>: Optionally, a <code class="language-plaintext highlighter-rouge">SummaryIndex</code> can be built to generate concise summaries of each document/collection. This is useful for providing quick overviews to the top-level agent or the user.</li> </ul> </li> <li> <p>Query Engines and Tools:</p> <ul> <li>The indices are exposed as query engines (e.g., <code class="language-plaintext highlighter-rouge">vector_index.as_query_engine()</code>, <code class="language-plaintext highlighter-rouge">summary_index.as_query_engine()</code>).</li> <li>These query engines are then wrapped into <code class="language-plaintext highlighter-rouge">QueryEngineTool</code> instances. Each tool is given a name and a description that outlines its specific capabilities (e.g., “Useful for answering specific factual questions about Collection X’s processing levels”).</li> </ul> </li> <li> <p><strong>Agent Definition</strong>: A <code class="language-plaintext highlighter-rouge">FunctionAgent</code> (often powered by an LLM like OpenAI’s <code class="language-plaintext highlighter-rouge">gpt-4o</code>) is instantiated for each collection. It’s equipped with the tools created above and a system prompt that directs it to use these tools to answer questions <em>exclusively</em> about its assigned data collection, avoiding reliance on prior knowledge. An asynchronous function <code class="language-plaintext highlighter-rouge">build_agent_per_doc</code> typically encapsulates this logic.</p> </li> </ul> <p><strong>3. Top-Level Orchestrator Agent:</strong></p> <ul> <li> <p><strong>Tool Aggregation</strong>: The specialized collection agents are exposed as tools to the top-level agent. This is done by wrapping the <code class="language-plaintext highlighter-rouge">agent.run</code> (or <code class="language-plaintext highlighter-rouge">agent.arun</code> for async) method of each collection agent into a <code class="language-plaintext highlighter-rouge">FunctionTool</code> using <code class="language-plaintext highlighter-rouge">FunctionTool.from_defaults</code>. The <code class="language-plaintext highlighter-rouge">description</code> of each <code class="language-plaintext highlighter-rouge">FunctionTool</code> is critical; it’s often derived from a summary of the collection the agent represents, enabling the top-level agent to make informed decisions about which tool (i.e., which collection agent) to engage for a given query.</p> </li> <li> <p>Tool Retrieval and Reranking :</p> <ol> <li>An <code class="language-plaintext highlighter-rouge">ObjectIndex.from_objects(all_tools, index_cls=VectorStoreIndex)</code> is created to index all the collection-agent tools.</li> <li>When the top-level agent receives a query, an initial set of relevant tools (collection agents) is retrieved using <code class="language-plaintext highlighter-rouge">obj_index.as_node_retriever(similarity_top_k=N)</code>.</li> <li>This retrieved set can be further refined using a postprocessor like <code class="language-plaintext highlighter-rouge">CohereRerank(top_n=M, model="rerank-english-v3.0")</code> (or similar) to improve the precision of tool selection.</li> <li>A <code class="language-plaintext highlighter-rouge">CustomObjectRetriever</code> (as demonstrated in LlamaIndex examples) can be implemented. This custom retriever can not only fetch the top N tools post-reranking but also dynamically inject a “comparison” or “query planning” sub-agent/tool. This sub-agent would take the original query and the selected tools as input, enabling it to explicitly compare information from multiple collections if the user’s query requires it (e.g., “Compare Sentinel-2 and Landsat 9 for vegetation monitoring”).</li> </ol> </li> <li> <p><strong>Query Execution</strong>: The top-level agent (e.g., a <code class="language-plaintext highlighter-rouge">FunctionAgent</code> or <code class="language-plaintext highlighter-rouge">ReActAgent</code> with an LLM like <code class="language-plaintext highlighter-rouge">gpt-4o</code>) then utilizes these selected and reranked tools (i.e., collection agents) to synthesize an answer to the user’s query. It effectively delegates sub-questions or information gathering tasks to the most relevant collection specialists.</p> </li> </ul> <p><strong>4. Output:</strong></p> <p>The system aims to identify the most suitable collection(s) for the user’s task. Furthermore, it can be prompted to extract key parameters (e.g., specific band names, asset keys for direct access, relevant processing levels, or even Python code snippets for data loading) based on the context retrieved by the RAG pipeline.</p> <p>Okay, I’ve revised the blog post to be more technical and included instructions for the flowchart.</p> <hr/> <h2 id="the-first-step-of-open-geoagent-a-technical-dive-into-finding-useful-geospatial-data-sources">The First Step of Open GeoAgent: A Technical Dive into Finding Useful Geospatial Data Sources</h2> <p>Efficiently discovering and preparing optimal geospatial datasets from heterogeneous, large-scale open repositories like Google Earth Engine (GEE), Microsoft Planetary Computer, NASA Earth Data, and AWS Open Data is a critical bottleneck for advanced geospatial analysis and AI applications. Open GeoAgent’s initial phase tackles this by implementing a robust system for discovering and preparing relevant geospatial data sources.</p> <p>This post details a two-stage architecture:</p> <ol> <li>A <strong>data processing pipeline</strong> converting STAC (SpatioTemporal Asset Catalog) metadata to GeoParquet, indexed by DuckDB for high-performance spatio-temporal querying.</li> <li>An <strong>intelligent retrieval layer</strong> using a multi-document Retrieval Augmented Generation (RAG) system built with LlamaIndex to identify task-specific collections and extract necessary parameters.</li> </ol> <h3 id="part-1-high-performance-data-preparation-with-stac-geoparquet-and-duckdb-1">Part 1: High-Performance Data Preparation with STAC, GeoParquet, and DuckDB</h3> <p>The foundation of our data discovery pipeline is the standardized STAC metadata. To optimize for analytical queries, STAC Items from target collections across major open data providers are transformed into GeoParquet files.</p> <p><strong>1. STAC to GeoParquet Transformation:</strong></p> <ul> <li><strong>Why GeoParquet?</strong> GeoParquet is an open, cloud-optimized, columnar format for geospatial vector data. Its columnar nature is key, allowing efficient storage and partial reads. This structure enables query engines like DuckDB to leverage predicate pushdown (filtering data at the source before reading it into memory) and columnar vectorization for significantly faster I/O and query processing. This is particularly advantageous compared to row-oriented formats or iterating through individual STAC items via an API for large-scale filtering.</li> <li><strong>Organization</strong>: Typically, each set of GeoParquet files corresponds to STAC items from a specific data collection, maintaining a clear and organized data lake structure. The schema of the GeoParquet files is derived directly from the STAC Item structure, including common metadata fields, asset links, and the geometry.</li> </ul> <p><strong>2. DuckDB for Accelerated Indexing and Querying:</strong></p> <p>DuckDB, an in-memory OLAP (Online Analytical Processing) DBMS, is employed for its exceptional speed, ease of integration (especially its Python bindings and direct Parquet reading capabilities), and rich SQL dialect.</p> <p>Key DuckDB features utilized:</p> <ul> <li> <p>Direct Parquet Querying: DuckDB can directly query one or more Parquet files, including those stored in cloud object storage.</p> <p>Python</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import duckdb
conn = duckdb.connect()
conn.sql("SELECT count(id) FROM 'path/to/your/collection_geoparquet/*.parquet';")
</code></pre></div> </div> </li> <li> <p>Spatial Extension: This extension is crucial for geospatial filtering. After installation (INSTALL spatial; LOAD spatial;), powerful spatial SQL operations can be performed.</p> <p>Python</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Example Python usage with DuckDB
import duckdb
  
# It's good practice to install and load extensions once per session if needed.
# For persistent storage, these might be set in the database configuration.
conn = duckdb.connect()
conn.execute("INSTALL spatial;")
conn.execute("LOAD spatial;")
  
# Define WKT for an Area of Interest (AOI) and time range
aoi_wkt = "POLYGON((-5.0 47.0, -5.0 48.0, -4.0 48.0, -4.0 47.0, -5.0 47.0))" # Example polygon
start_time = "2023-03-01T00:00:00Z"
end_time = "2023-08-31T23:59:59Z"
  
query = f"""
SELECT id, properties_datetime, assets_B04_href, properties_eo_cloud_cover
FROM read_parquet('path_to_your_geoparquet_files/*.parquet', union_by_name=True)
WHERE "sar:product_type" = 'GRD' -- Example for Sentinel-1
  AND "sar:instrument_mode" = 'IW' -- Example for Sentinel-1
  AND ST_Intersects(geometry, ST_GeomFromText('{aoi_wkt}'))
  AND properties_datetime &gt;= '{start_time}'
  AND properties_datetime &lt;= '{end_time}'
  AND properties_eo_cloud_cover &lt; 20; -- Example cloud cover filter
"""
result = conn.execute(query).fetchdf()
print(result.head())
</code></pre></div> </div> </li> <li> <p>CQL2 to SQL Translation: To maintain compatibility with existing STAC API workflows and allow users to leverage familiar query languages, the</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pygeofilter
</code></pre></div> </div> <p>library, along with its</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pygeofilter-duckdb
</code></pre></div> </div> <p>backend, can parse CQL2-JSON filters and translate them into DuckDB SQL</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>WHERE
</code></pre></div> </div> <p>clauses. This allows users to define filters once and apply them to both STAC APIs and the GeoParquet/DuckDB backend.</p> <p>Python</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Conceptual Python usage with pygeofilter
from pygeofilter.parsers.cql2_json import parse as json_parse
from pygeofilter.backends.duckdb import to_sql_where
from pygeofilter.util import IdempotentDict
  
cql2_filter = {
  "op": "and",
  "args": [
    {"op": "between", "args": [{"property": "eo:cloud_cover"}, 0, 21]},
    {"op": "between", "args": [{"property": "datetime"}, "2023-02-01T00:00:00Z", "2023-02-28T23:59:59Z"]},
    {"op": "s_intersects", "args": [{"property": "geometry"}, {"type": "Polygon", "coordinates": [[[...]]]}]}
  ]
}
# field_mapping can be used if property names differ from GeoParquet column names
sql_where_clause = to_sql_where(json_parse(cql2_filter), IdempotentDict())
# full_query = f"SELECT * FROM read_parquet('{geoparquet_path}') WHERE {sql_where_clause}"
</code></pre></div> </div> </li> <li> <dl> <dt>STAC Item Reconstruction</dt> <dd> <p>Query results from DuckDB (often retrieved as Arrow tables or Pandas DataFrames) can be efficiently converted back to</p> </dd> </dl> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pystac.Item
</code></pre></div> </div> <p>objects using the</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>stac_geoparquet.arrow._api.stac_table_to_items
</code></pre></div> </div> <p>function. This ensures seamless integration with downstream STAC-aware tools and libraries.</p> <p>Python</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Conceptual Python snippet (based on your provided material)
# import json
# import pystac
# from stac_geoparquet.arrow._api import stac_table_to_items
# db_result_arrow_table = conn.execute(full_query).fetch_arrow_table()
# stac_items = []
# for item_dict in stac_table_to_items(db_result_arrow_table):
#     item_dict['assets'] = json.loads(item_dict['assets']) # Assets might be stored as JSON strings
#     stac_items.append(pystac.Item.from_dict(item_dict))
</code></pre></div> </div> </li> </ul> <p>This spatio-temporal pre-filtering significantly prunes the search space, making the subsequent semantic retrieval step more focused and computationally feasible.</p> <h3 id="part-2-intelligent-collection-selection-via-multi-document-rag-with-llamaindex-1">Part 2: Intelligent Collection Selection via Multi-Document RAG with LlamaIndex</h3> <p>Even after spatio-temporal filtering, multiple data collections might meet the basic criteria. Selecting the <em>most suitable</em> collection for a nuanced task (e.g., “which LiDAR dataset is best for sub-meter vertical accuracy canopy height models in a temperate forest region?”) requires deeper semantic understanding of each collection’s specifications, processing levels, and suitability for specific applications. A Multi-Document Retrieval Augmented Generation (RAG) system, built using LlamaIndex, provides this capability.</p> <p><strong>1. Architecture Overview:</strong></p> <p>The system employs a two-tiered agent structure:</p> <ul> <li><strong>Collection-Specific Agents (<code class="language-plaintext highlighter-rouge">FunctionAgent</code>)</strong>: Each candidate geospatial data collection, represented by its detailed metadata, user guides, scientific papers describing it, or even API documentation, is managed by a dedicated LlamaIndex <code class="language-plaintext highlighter-rouge">FunctionAgent</code>.</li> <li><strong>Top-Level Orchestrator Agent (<code class="language-plaintext highlighter-rouge">FunctionAgent</code> or <code class="language-plaintext highlighter-rouge">ReActAgent</code>)</strong>: This agent receives the user’s primary query and intelligently routes sub-queries to the appropriate collection-specific agents.</li> </ul> <p><strong>2. Building Collection-Specific Agents:</strong></p> <p>The process, inspired by LlamaIndex’s multi-document agent patterns, involves creating specialized agents for each data collection:</p> <ul> <li> <p><strong>Document Processing</strong>: For each collection, relevant documents (e.g., landing pages, technical specifications, usage tutorials) are loaded and parsed into nodes using <code class="language-plaintext highlighter-rouge">SentenceSplitter</code>.</p> </li> <li> <p>Index Creation</p> <p>:</p> <ul> <li><code class="language-plaintext highlighter-rouge">VectorStoreIndex</code>: Built from these nodes using an embedding model (e.g., <code class="language-plaintext highlighter-rouge">OpenAIEmbedding(model="text-embedding-3-small")</code>). This index allows for semantic search within the collection’s documentation (e.g., “find information about radiometric correction”).</li> <li><code class="language-plaintext highlighter-rouge">SummaryIndex</code>: Optionally, a <code class="language-plaintext highlighter-rouge">SummaryIndex</code> can be built to generate concise summaries of each document/collection. This is useful for providing quick overviews to the top-level agent or the user.</li> </ul> </li> <li> <p>Query Engines and Tools</p> <p>:</p> <ul> <li>The indices are exposed as query engines (e.g., <code class="language-plaintext highlighter-rouge">vector_index.as_query_engine()</code>, <code class="language-plaintext highlighter-rouge">summary_index.as_query_engine()</code>).</li> <li>These query engines are then wrapped into <code class="language-plaintext highlighter-rouge">QueryEngineTool</code> instances. Each tool is given a name and a description that outlines its specific capabilities (e.g., “Useful for answering specific factual questions about Collection X’s processing levels”).</li> </ul> </li> <li> <p><strong>Agent Definition</strong>: A <code class="language-plaintext highlighter-rouge">FunctionAgent</code> (often powered by an LLM like OpenAI’s <code class="language-plaintext highlighter-rouge">gpt-4o</code>) is instantiated for each collection. It’s equipped with the tools created above and a system prompt that directs it to use these tools to answer questions <em>exclusively</em> about its assigned data collection, avoiding reliance on prior knowledge. An asynchronous function <code class="language-plaintext highlighter-rouge">build_agent_per_doc</code> typically encapsulates this logic.</p> </li> </ul> <p><strong>3. Top-Level Orchestrator Agent:</strong></p> <ul> <li> <p><strong>Tool Aggregation</strong>: The specialized collection agents are exposed as tools to the top-level agent. This is done by wrapping the <code class="language-plaintext highlighter-rouge">agent.run</code> (or <code class="language-plaintext highlighter-rouge">agent.arun</code> for async) method of each collection agent into a <code class="language-plaintext highlighter-rouge">FunctionTool</code> using <code class="language-plaintext highlighter-rouge">FunctionTool.from_defaults</code>. The <code class="language-plaintext highlighter-rouge">description</code> of each <code class="language-plaintext highlighter-rouge">FunctionTool</code> is critical; it’s often derived from a summary of the collection the agent represents, enabling the top-level agent to make informed decisions about which tool (i.e., which collection agent) to engage for a given query.</p> </li> <li> <p>Tool Retrieval and Reranking :</p> <ol> <li>An <code class="language-plaintext highlighter-rouge">ObjectIndex.from_objects(all_tools, index_cls=VectorStoreIndex)</code> is created to index all the collection-agent tools.</li> <li>When the top-level agent receives a query, an initial set of relevant tools (collection agents) is retrieved using <code class="language-plaintext highlighter-rouge">obj_index.as_node_retriever(similarity_top_k=N)</code>.</li> <li>This retrieved set can be further refined using a postprocessor like <code class="language-plaintext highlighter-rouge">CohereRerank(top_n=M, model="rerank-english-v3.0")</code> (or similar) to improve the precision of tool selection.</li> <li>A <code class="language-plaintext highlighter-rouge">CustomObjectRetriever</code> (as demonstrated in LlamaIndex examples) can be implemented. This custom retriever can not only fetch the top N tools post-reranking but also dynamically inject a “comparison” or “query planning” sub-agent/tool. This sub-agent would take the original query and the selected tools as input, enabling it to explicitly compare information from multiple collections if the user’s query requires it (e.g., “Compare Sentinel-2 and Landsat 9 for vegetation monitoring”).</li> </ol> </li> <li> <p><strong>Query Execution</strong>: The top-level agent (e.g., a <code class="language-plaintext highlighter-rouge">FunctionAgent</code> or <code class="language-plaintext highlighter-rouge">ReActAgent</code> with an LLM like <code class="language-plaintext highlighter-rouge">gpt-4o</code>) then utilizes these selected and reranked tools (i.e., collection agents) to synthesize an answer to the user’s query. It effectively delegates sub-questions or information gathering tasks to the most relevant collection specialists.</p> </li> </ul> <p><strong>4. Output:</strong></p> <p>The system aims to identify the most suitable collection(s) for the user’s task. Furthermore, it can be prompted to extract key parameters (e.g., specific band names, asset keys for direct access, relevant processing levels, or even Python code snippets for data loading) based on the context retrieved by the RAG pipeline.</p> <h3 id="conclusion-and-future-work">Conclusion and Future Work</h3> <p>This two-stage architecture – combining high-performance pre-filtering with DuckDB/GeoParquet and sophisticated semantic retrieval with LlamaIndex RAG – forms a robust and technically sound first step for Open GeoAgent. It addresses the dual challenges of massive data volume and the need for nuanced semantic relevance in data discovery, paving the way for more automated, intelligent, and insightful geospatial data utilization. Future enhancements will focus on loading data from queried data collection to local data analysis.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="LLMs,"/><category term="Remote"/><category term="Sensing"/><summary type="html"><![CDATA[how to use RAG on open earth data]]></summary></entry><entry><title type="html">A Preliminary Study of Visual Prompt-Based Open-Vocabulary RSI Segmentation and Change Detection in VLMs</title><link href="https://yusin2chen.github.io/blog/2023/VQA/" rel="alternate" type="text/html" title="A Preliminary Study of Visual Prompt-Based Open-Vocabulary RSI Segmentation and Change Detection in VLMs"/><published>2023-08-15T21:01:00+00:00</published><updated>2023-08-15T21:01:00+00:00</updated><id>https://yusin2chen.github.io/blog/2023/VQA</id><content type="html" xml:base="https://yusin2chen.github.io/blog/2023/VQA/"><![CDATA[<h2 id="abstract">abstract</h2> <p>Large visual-language models (VLMs) have demonstrated robust capabilities in zero-shot classification and information retrieval in remote sensing (RS) images. Nevertheless, their capability is presently constrained to pixel-dense tasks like semantic segmentation and pixel-wise change detection, which have to be done in a particular context in various applications. To address this limitation, this study investigates the utilization of visual prompts combined with textual inputs to tackle open-vocabulary semantic segmentation and change detection tasks on RS images. Specifically, the red boundary of each object obtained from the Segment Anything Model is employed as the visual prompt and to guide the VLM’s focus towards the target of interest while preserving contextual information from the surroundings. Our experimental results demonstrate the potential of the proposed approach, showcasing impressive zero-shot performance and robust reasoning abilities in the realm of RS image open-vocabulary segmentation and change detection tasks.</p> <h2 id="introduction">Introduction</h2> <p>Remote sensing (RS) images encompassing diverse modalities and resolutions contain a wealth of information on land cover and land use, including high-level information about the relationships between different objects. However, the existing techniques for extracting this information are typically tailored to specific tasks and trained on a fixed set of object classes, thus limiting the flexibility in accessing information from RS data. Consequently, the remote sensing community has become increasingly intrigued by the zero-shot capability offered by visual-language models (VLMs [1].). Recently, Chen et al. introduced a conditional Unet model, which utilizes a pre-trained CLIP model and OpenStreetMap (OSM) data to predict segmentation masks based on text descriptions. Although this text-conditioned model allows for the segmentation of interest objects based on textual prompts, its application is restricted to a few types of object annotations from OpenStreetMap and fails to perform detailed inquiries from the images. Seeking interactive dialogue ability, Lobry et al.[2] developed the first visual question answering (VQA) dataset, involving queries about selected areas from OSM integrated with Sentinel-2 images. Moreover, Yuan et al. [3] employed a VQA system for change detection on multitemporal aerial images by generating question-answer pairs using an automatic method. To further advance the capabilities of RS vision-language models, Zhan et al. [4] curated the RS images-text dataset (RSVG) to tackle the visual grounding task on RS data, which comprises a substantial number of language expressions paired with RS images and object categories. However, it was discovered that vision-language models trained on general VQA and captioning tasks exhibit limited visual reasoning abilities and lack the appropriate visual prompt for dense-pixel downstream tasks. To address these limitations, our proposal involves leveraging segmentation results from the Segment Anything Model as a visual prompt, combined with text prompts, to extract useful information, such as object category and change status, from large VLMs in a zero-shot manner. Notably, we demonstrate that using a red boundary as the visual prompt guides the VLM to analyze or talk about the interest object more effectively than only cropping, as it preserves contextual information. We showcase the potential of zero-shot performance in RS image open-vocabulary segmentation and change detection. Moreover, to overcome challenges in identifying small objects of different scales with context and information queries.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/flowchart-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/flowchart-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/flowchart-1400.webp"/> <img src="/assets/img/flowchart.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig. 1 Overview of the visual prompt guided open vocabulary remote sensing image semantic segmentation and change detection. (a) We use the Segment Anything Model to get the boundary of each object in the image as our visual prompt. (b) The object-centric image crop with a red boundary around the object is inputted to the VLM with a text prompt for extracting useful information. (c) The text prompt for semantic information query, customized classification and object-based change detection. </div> <h2 id="methodology">Methodology</h2> <h3 id="segment-anything-model">Segment Anything Model</h3> <p>The Segment Anything Model (SAM) is a novel image segmentation tool that was trained using the largest available segmentation dataset. This model can handle both interactive and automatic segmentation tasks. A noteworthy feature of SAM is its promptable interface, affording versatility in its application, as it can be tailored to various segmentation tasks through the appropriate engineering of prompts, such as clicks, boxes, or text. The model’s training is based on an extensive and high-quality dataset comprising over 1 billion masks, meticulously assembled as part of the project. This extensive training empowers the model to demonstrate robust generalization capabilities, enabling it to effectively handle new object types and image scenarios beyond its original training data. Consequently, this capacity to generalize alleviates the need for practitioners to amass their own segmentation data and perform fine-tuning for specific use cases. The SAM architecture is composed of three core components: an image encoder, a flexible prompt encoder, and a fast mask decoder. The image encoder leverages a Vision Transformer (ViT) backbone, capable of processing high-resolution 1024 × 1024 images and generating an image embedding of spatial size 64 × 64. On the other hand, the prompt encoder accepts sparse prompts, encompassing points, boxes, and text, as well as dense prompts, like masks. It skillfully translates these prompts into c-dimensional tokens. Lastly, the lightweight mask decoder seamlessly integrates the image and prompt embeddings, efficiently predicting segmentation masks in real-time, thus enabling SAM to adapt gracefully to diverse prompts while incurring minimal computational overhead.</p> <h3 id="vison-language-models">Vison-Language Models</h3> <p>Large language models (LLMs) have demonstrated remarkable reasoning capabilities in diverse tasks, including chatbot interactions, mathematical calculations, and code generation. Nonetheless, a significant limitation of these models lies in their inability to effectively infer from real-world visual inputs. Generative vision language models, on the other hand, capitalize on pre-trained large language models’ potential to comprehend both images and text, thereby showcasing their prowess in visual reasoning tasks. These generative models excel in tasks like visual question answering, image captioning, optical character recognition, and object detection. For instance, FLAMINGO and BLIP [2,5] are notable examples of generative VLMs, excelling in captioning and visual question-answering tasks. However, they fall short when confronted with pixel-level computer vision tasks. Training a large multimodal language model (VLM) end-to-end to bridge the gap between LLMs and images can be a computationally expensive endeavor. To overcome this challenge, the prevalent approach for VLMs involves introducing a learnable interface, utilizing frozen pre-trained LLM models between the pre-trained visual encoder and LLM. By employing a set of learned query tokens, these models can extract information through a query-based mechanism, with the vision encoder parameters optimized via backpropagation through a frozen LLM (such as Flamingo, BLIP, LLaVA). In the scope of this study, we investigate the PaLM-E and LLaVA as the Vision-language Model backbone. These models incorporate visual inputs into the same latent embedding as language tokens and process them using self-attention layers within a Transformer-based LLM, similar to how text inputs are treated. The inputs to PaLM-E comprise both textual and (multiple) visual observations. Consequently, we can perform image segmentation on a single visual input or leverage multiple visual inputs for change detection tasks in the context of remote sensing images. An illustrative example of such usage is a question posed as follows: “What events occurred between \(&lt;img1&gt;\) and \(&lt;img2&gt;\)?”, where \(&lt;img_i&gt;\) represents the image acquired at time \(i\).</p> <h3 id="visual-prompt">Visual Prompt</h3> <p>In the context of prompting VLMs, a prevalent technique involves incorporating a set of learnable tokens into either the text input or the visual inputs. This approach effectively guides a frozen VLM to perform zero-shot inference on downstream tasks. Commonly employed visual prompts include pixel space augmentation techniques, such as image padding, patch modification, or image inpainting. Another notable strategy known as “Colorful Prompt Tuning” entails highlighting color regions within an image and using a captioning model to predict which object in the image a given expression refers to, based on its color. For instance, Shtedritski et al. [5] utilized a simple red circle drawn around an object in the image to serve as a visual prompt. In line with this concept, our method involves annotating a red enclosed boundary around the designated object, demonstrating that our approach surpasses the effectiveness and flexibility of a mere circular or rectangular bounding box. This red enclosed boundary acts as a powerful and versatile visual prompt, facilitating enhanced interactions with the VLM and enabling more accurate and context-aware zero-shot inferences.</p> <h2 id="experimental-results">Experimental Results</h2> <h3 id="evaluation-setting">Evaluation Setting</h3> <p>In our investigation of open-vocabulary semantic segmentation and change detection, we utilize data sourced from OSM for the purpose of constructing our evaluation dataset. OSM is an openly accessible repository housing geolocalized information that is contributed by volunteers. Leveraging this data, we are able to automatically extract the relevant information, which subsequently aids in generating text descriptions and semantic segmentation maps. The dataset construction process involves several steps. Firstly, we identify the objects of interest and proceed to filter recognizable objects from the land cover and land use layer available in OSM. Examples of such objects include “silos,” “apartments,” “roads,” and others. Subsequently, the vector annotations of these selected objects are rasterized into tiles. By querying all objects that fall within each tile, we obtain a comprehensive semantic segmentation map encompassing the entirety of relevant objects. Furthermore, we undertake a careful examination of changes in various objects by considering the bi-temporal images and the change labels provided by OSM. For semantic segmentation, we consider the semantic segment anything model as a comparison [6] in the future, which is based on the Segment Anything Model and gets the rich vocabulary semantic information from CLIPSeg, BLIP and CLIP. For change detection, we consider the CDVQA as a potential comparison, which only provides the text description of changes.</p> <h3 id="results">Results</h3> <p>We first evaluate the results of semantic segmentation considering various object sizes. In Fig. 2, we present RGB images alongside segmentations obtained from SAM and semantic information derived from the VLM. Notably, to ensure contextual relevance and informative details, we eliminated excessively small and large objects based on image size. This step is essential as object information cannot be reliably inferred without adequate context or sufficient details. Regarding semantic information, square-shaped objects exhibit superior results compared to slim and elongated objects. The segmentation of slim objects includes more unrelated context, which confuses the VLM’s understanding. Furthermore, the VLM struggles with small objects that lack detailed information within the given image resolution. Additionally, the VLM may even yield unrelated information when encountering objects with shadows. For change detection (Fig. 3), our approach involves obtaining segmentations of bi-temporal images using SAM. Subsequently, we remove all overlapped objects, indicating no changes have occurred. The remaining objects, accompanied by visual prompts, are then fed into the VLM to identify their object types. This serves as context for text prompts to query whether changes happened or not. It is worth noting that the changing status for different objects varies; for example, a parking lot may remain unchanged with or without cars, but changes can still occur in other aspects. In the realm of change detection, the selection of an appropriate text prompt is critical. Generally, we observed that comparing segmentations from bi-temporal images effectively detects the most changed objects. Any remaining false alarms can be mitigated by querying the VLM, particularly in complex scenes. This process assists in refining the accuracy of change detection results.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/segmentation-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/segmentation-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/segmentation-1400.webp"/> <img src="/assets/img/segmentation.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig. 2 Examples of semantic segmentation results obtained by SAM and Google Bard, organized in three scenes. Col. 1: RGB images, Col. 2: segmentations from SAM, Col. 3: semantic information from Google Bard. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/change%20detection-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/change%20detection-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/change%20detection-1400.webp"/> <img src="/assets/img/change%20detection.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig. 3 Examples of change detection results obtained by SAM and Google Bard. Col. 1: pre-event images, Col. 2: post-event images, Col. 3: objects with change information. </div> <h2 id="conclusion">Conclusion</h2> <p>In this study, we have shown that the visual prompt from SAM can extract useful semantic information and change statu information from VLMs such as Google Bard and LLaVA in a zero-shot manner, achieving the zero-shot open-vocabulary remote sensing image segmentation and change detection performance. Our comprehensive analysis highlights that the visual prompt and reasoning capabilities of these large multimodal language models can effectively extract pertinent information from complex geospatial scenes. However, it is evident that models trained solely on natural images exhibit certain limitations when applied to remote sensing images. To further enhance the performance of our approach, it is imperative to conduct additional fine-tuning of the SAM and VLMs on remote sensing images. Preliminary findings indicate the need for this fine-tuning process. In our future research endeavors, we envision incorporating diverse sensor data, such as synthetic-aperture radar (SAR), multispectral, and hyperspectral images, in order to enhance the inference performance of our approach and better accommodate the unique characteristics of remote sensing data.</p> <h2 id="references">References</h2> <p>[1] C. Wen, Y. Hu, X. Li, Z. Yuan, and X. X. Zhu, “Vision-language models in remote sensing: Current progress and future trends,” arXiv preprint arXiv:2305.05726, 2023.</p> <p>[2] S. Lobry, D. Marcos, J. Murray, and D. Tuia, “Rsvqa: Visual question answering for remote sensing data,” IEEE Transactions on Geoscience and Remote Sensing, vol. 58, no. 12, pp. 8555–8566, 2020.</p> <p>[3] Z. Yuan, L. Mou, Z. Xiong, and X. X. Zhu, “Change detection meets visual question answering,” IEEE Transactions on Geoscience and Remote Sensing, vol. 60, pp. 1–13, 2022.</p> <p>[4] Y. Zhan, Z. Xiong, and Y. Yuan, “Rsvg: Exploring data and models for visual grounding on remote sensing data,” arXiv preprint arXiv:2210.12634, 2022.</p> <p>[5] A. Shtedritski, C. Rupprecht, and A. Vedaldi, “What does clip know about a red circle? visual prompt engineering for vlms,” arXiv preprint arXiv:2304.06712, 2023.</p> <p>[6] J. Chen, Z. Yang, and L. Zhang, “Semantic segment anything,” https://github.com/fudan-zvg/Semantic-Segment-Anything, 2023</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="LLMs,"/><category term="Remote"/><category term="Sensing"/><summary type="html"><![CDATA[how to use LLM for RS VQA tasks]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://yusin2chen.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://yusin2chen.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://yusin2chen.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">a post with diagrams</title><link href="https://yusin2chen.github.io/blog/2021/diagrams/" rel="alternate" type="text/html" title="a post with diagrams"/><published>2021-07-04T17:39:00+00:00</published><updated>2021-07-04T17:39:00+00:00</updated><id>https://yusin2chen.github.io/blog/2021/diagrams</id><content type="html" xml:base="https://yusin2chen.github.io/blog/2021/diagrams/"><![CDATA[<p>This theme supports generating various diagrams from a text description using <a href="https://github.com/zhustec/jekyll-diagrams" target="\_blank">jekyll-diagrams</a> plugin. Below, we generate a few examples of such diagrams using languages such as <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid</a>, <a href="https://plantuml.com/" target="\_blank">plantuml</a>, <a href="https://vega.github.io/vega-lite/" target="\_blank">vega-lite</a>, etc.</p> <p><strong>Note:</strong> different diagram-generation packages require external dependencies to be installed on your machine. Also, be mindful of that because of diagram generation the fist time you build your Jekyll website after adding new diagrams will be SLOW. For any other details, please refer to <a href="https://github.com/zhustec/jekyll-diagrams" target="\_blank">jekyll-diagrams</a> README.</p> <h2 id="mermaid">Mermaid</h2> <p>Install mermaid using <code class="language-plaintext highlighter-rouge">node.js</code> package manager <code class="language-plaintext highlighter-rouge">npm</code> by running the following command:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>npm <span class="nb">install</span> <span class="nt">-g</span> mermaid.cli
</code></pre></div></div> <p>The diagram below was generated by the following code:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% mermaid %}
sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
{% endmermaid %}
</code></pre></div></div> <div class="jekyll-diagrams diagrams mermaid"> <svg id="mermaid-1749767395286" width="100%" xmlns="http://www.w3.org/2000/svg" height="100%" style="max-width:450px;" viewBox="-50 -10 450 231"><style>#mermaid-1749767395286 .label{font-family:trebuchet ms,verdana,arial;color:#333}#mermaid-1749767395286 .node circle,#mermaid-1749767395286 .node ellipse,#mermaid-1749767395286 .node polygon,#mermaid-1749767395286 .node rect{fill:#ececff;stroke:#9370db;stroke-width:1px}#mermaid-1749767395286 .node.clickable{cursor:pointer}#mermaid-1749767395286 .arrowheadPath{fill:#333}#mermaid-1749767395286 .edgePath .path{stroke:#333;stroke-width:1.5px}#mermaid-1749767395286 .edgeLabel{background-color:#e8e8e8}#mermaid-1749767395286 .cluster rect{fill:#ffffde!important;stroke:#aa3!important;stroke-width:1px!important}#mermaid-1749767395286 .cluster text{fill:#333}#mermaid-1749767395286 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:trebuchet ms,verdana,arial;font-size:12px;background:#ffffde;border:1px solid #aa3;border-radius:2px;pointer-events:none;z-index:100}#mermaid-1749767395286 .actor{stroke:#ccf;fill:#ececff}#mermaid-1749767395286 text.actor{fill:#000;stroke:none}#mermaid-1749767395286 .actor-line{stroke:grey}#mermaid-1749767395286 .messageLine0{marker-end:"url(#arrowhead)"}#mermaid-1749767395286 .messageLine0,#mermaid-1749767395286 .messageLine1{stroke-width:1.5;stroke-dasharray:"2 2";stroke:#333}#mermaid-1749767395286 #arrowhead{fill:#333}#mermaid-1749767395286 #crosshead path{fill:#333!important;stroke:#333!important}#mermaid-1749767395286 .messageText{fill:#333;stroke:none}#mermaid-1749767395286 .labelBox{stroke:#ccf;fill:#ececff}#mermaid-1749767395286 .labelText,#mermaid-1749767395286 .loopText{fill:#000;stroke:none}#mermaid-1749767395286 .loopLine{stroke-width:2;stroke-dasharray:"2 2";marker-end:"url(#arrowhead)";stroke:#ccf}#mermaid-1749767395286 .note{stroke:#aa3;fill:#fff5ad}#mermaid-1749767395286 .noteText{fill:#000;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:14px}#mermaid-1749767395286 .section{stroke:none;opacity:.2}#mermaid-1749767395286 .section0{fill:rgba(102,102,255,.49)}#mermaid-1749767395286 .section2{fill:#fff400}#mermaid-1749767395286 .section1,#mermaid-1749767395286 .section3{fill:#fff;opacity:.2}#mermaid-1749767395286 .sectionTitle0,#mermaid-1749767395286 .sectionTitle1,#mermaid-1749767395286 .sectionTitle2,#mermaid-1749767395286 .sectionTitle3{fill:#333}#mermaid-1749767395286 .sectionTitle{text-anchor:start;font-size:11px;text-height:14px}#mermaid-1749767395286 .grid .tick{stroke:#d3d3d3;opacity:.3;shape-rendering:crispEdges}#mermaid-1749767395286 .grid path{stroke-width:0}#mermaid-1749767395286 .today{fill:none;stroke:red;stroke-width:2px}#mermaid-1749767395286 .task{stroke-width:2}#mermaid-1749767395286 .taskText{text-anchor:middle;font-size:11px}#mermaid-1749767395286 .taskTextOutsideRight{fill:#000;text-anchor:start;font-size:11px}#mermaid-1749767395286 .taskTextOutsideLeft{fill:#000;text-anchor:end;font-size:11px}#mermaid-1749767395286 .taskText0,#mermaid-1749767395286 .taskText1,#mermaid-1749767395286 .taskText2,#mermaid-1749767395286 .taskText3{fill:#fff}#mermaid-1749767395286 .task0,#mermaid-1749767395286 .task1,#mermaid-1749767395286 .task2,#mermaid-1749767395286 .task3{fill:#8a90dd;stroke:#534fbc}#mermaid-1749767395286 .taskTextOutside0,#mermaid-1749767395286 .taskTextOutside1,#mermaid-1749767395286 .taskTextOutside2,#mermaid-1749767395286 .taskTextOutside3{fill:#000}#mermaid-1749767395286 .active0,#mermaid-1749767395286 .active1,#mermaid-1749767395286 .active2,#mermaid-1749767395286 .active3{fill:#bfc7ff;stroke:#534fbc}#mermaid-1749767395286 .activeText0,#mermaid-1749767395286 .activeText1,#mermaid-1749767395286 .activeText2,#mermaid-1749767395286 .activeText3{fill:#000!important}#mermaid-1749767395286 .done0,#mermaid-1749767395286 .done1,#mermaid-1749767395286 .done2,#mermaid-1749767395286 .done3{stroke:grey;fill:#d3d3d3;stroke-width:2}#mermaid-1749767395286 .doneText0,#mermaid-1749767395286 .doneText1,#mermaid-1749767395286 .doneText2,#mermaid-1749767395286 .doneText3{fill:#000!important}#mermaid-1749767395286 .crit0,#mermaid-1749767395286 .crit1,#mermaid-1749767395286 .crit2,#mermaid-1749767395286 .crit3{stroke:#f88;fill:red;stroke-width:2}#mermaid-1749767395286 .activeCrit0,#mermaid-1749767395286 .activeCrit1,#mermaid-1749767395286 .activeCrit2,#mermaid-1749767395286 .activeCrit3{stroke:#f88;fill:#bfc7ff;stroke-width:2}#mermaid-1749767395286 .doneCrit0,#mermaid-1749767395286 .doneCrit1,#mermaid-1749767395286 .doneCrit2,#mermaid-1749767395286 .doneCrit3{stroke:#f88;fill:#d3d3d3;stroke-width:2;cursor:pointer;shape-rendering:crispEdges}#mermaid-1749767395286 .activeCritText0,#mermaid-1749767395286 .activeCritText1,#mermaid-1749767395286 .activeCritText2,#mermaid-1749767395286 .activeCritText3,#mermaid-1749767395286 .doneCritText0,#mermaid-1749767395286 .doneCritText1,#mermaid-1749767395286 .doneCritText2,#mermaid-1749767395286 .doneCritText3{fill:#000!important}#mermaid-1749767395286 .titleText{text-anchor:middle;font-size:18px;fill:#000}
#mermaid-1749767395286 g.classGroup text{fill:#9370db;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:10px}#mermaid-1749767395286 g.classGroup rect{fill:#ececff;stroke:#9370db}#mermaid-1749767395286 g.classGroup line{stroke:#9370db;stroke-width:1}#mermaid-1749767395286 .classLabel .box{stroke:none;stroke-width:0;fill:#ececff;opacity:.5}#mermaid-1749767395286 .classLabel .label{fill:#9370db;font-size:10px}#mermaid-1749767395286 .relation{stroke:#9370db;stroke-width:1;fill:none}#mermaid-1749767395286 #compositionEnd,#mermaid-1749767395286 #compositionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1749767395286 #aggregationEnd,#mermaid-1749767395286 #aggregationStart{fill:#ececff;stroke:#9370db;stroke-width:1}#mermaid-1749767395286 #dependencyEnd,#mermaid-1749767395286 #dependencyStart,#mermaid-1749767395286 #extensionEnd,#mermaid-1749767395286 #extensionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1749767395286 .branch-label,#mermaid-1749767395286 .commit-id,#mermaid-1749767395286 .commit-msg{fill:#d3d3d3;color:#d3d3d3}</style><style>#mermaid-1749767395286{color:#000;font:normal normal 400 normal 16px / normal "Times New Roman"}</style><g></g><g><line id="actor0" x1="75" y1="5" x2="75" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="0" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><line id="actor1" x1="275" y1="5" x2="275" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="200" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g><defs><marker id="arrowhead" refX="5" refY="2" markerWidth="6" markerHeight="4" orient="auto"><path d="M 0,0 V 4 L6,2 Z"></path></marker></defs><defs><marker id="crosshead" markerWidth="15" markerHeight="8" orient="auto" refX="16" refY="4"><path fill="black" stroke="#000000" stroke-width="1px" d="M 9,2 V 6 L16,4 Z" style="stroke-dasharray: 0, 0;"></path><path fill="none" stroke="#000000" stroke-width="1px" d="M 0,1 L 6,7 M 6,1 L 0,7" style="stroke-dasharray: 0, 0;"></path></marker></defs><g><text x="175" y="93" class="messageText" style="text-anchor: middle;">Hello John, how are you?</text><line x1="275" y1="100" x2="75" y2="100" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="175" y="128" class="messageText" style="text-anchor: middle;">Great!</text><line x1="75" y1="135" x2="275" y2="135" class="messageLine1" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="stroke-dasharray: 3, 3; fill: none;"></line></g><g><rect x="0" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><rect x="200" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g></svg> </div>]]></content><author><name></name></author><category term="formatting"/><category term="diagrams"/><summary type="html"><![CDATA[an example of a blog post with diagrams]]></summary></entry></feed>