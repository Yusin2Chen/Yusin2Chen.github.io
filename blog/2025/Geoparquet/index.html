<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>The First Step of Open GeoAgent - A Technical Dive into Finding Useful Geospatial Data Sources | BeyondEarth</title> <meta name="author" content="Yusin Chen"> <meta name="description" content="how to use RAG on open earth data"> <meta name="keywords" content="remote sensing, geospatial, planetary data knowledge discovery"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yusin2chen.github.io/blog/2025/Geoparquet/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">BeyondEarth</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Me</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">learning</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">more</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/remote_sensing/">remote_sensing</a> <a class="dropdown-item" href="/intelligence/">intelligence</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">The First Step of Open GeoAgent - A Technical Dive into Finding Useful Geospatial Data Sources</h1> <p class="post-meta">May 27, 2025</p> <p class="post-tags"> <a href="/blog/2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/llms"> <i class="fas fa-hashtag fa-sm"></i> LLMs,</a>   <a href="/blog/tag/remote"> <i class="fas fa-hashtag fa-sm"></i> Remote</a>   <a href="/blog/tag/sensing"> <i class="fas fa-hashtag fa-sm"></i> Sensing</a>     ·   <a href="/blog/category/sample-posts"> <i class="fas fa-tag fa-sm"></i> sample-posts</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="abstract">abstract</h2> <p>Efficiently discovering and preparing optimal geospatial datasets from heterogeneous, large-scale open repositories like Google Earth Engine (GEE), Microsoft Planetary Computer, NASA Earth Data, and AWS Open Data is a critical bottleneck for advanced geospatial analysis and AI applications. Open GeoAgent’s initial phase tackles this by implementing a robust system for discovering and preparing relevant geospatial data sources.</p> <p>This post details a two-stage architecture:</p> <ol> <li>A <strong>data processing pipeline</strong> converting STAC (SpatioTemporal Asset Catalog) metadata to GeoParquet, indexed by DuckDB for high-performance spatio-temporal querying.</li> <li>An <strong>intelligent retrieval layer</strong> using a multi-document Retrieval Augmented Generation (RAG) system built with LlamaIndex to identify task-specific collections and extract necessary parameters.</li> <li>scripts and geoparquet data link: https://huggingface.co/datasets/Yusin/GeoAgent-Geoparquet</li> </ol> <h2 id="part-1-high-performance-data-preparation-with-stac-geoparquet-and-duckdb">Part 1: High-Performance Data Preparation with STAC, GeoParquet, and DuckDB</h2> <p>The foundation of our data discovery pipeline is the standardized STAC metadata. To optimize for analytical queries, STAC Items from target collections across major open data providers are transformed into GeoParquet files.</p> <p><strong>1. STAC to GeoParquet Transformation:</strong></p> <ul> <li> <strong>Why GeoParquet?</strong> GeoParquet is an open, cloud-optimized, columnar format for geospatial vector data. Its columnar nature is key, allowing efficient storage and partial reads. This structure enables query engines like DuckDB to leverage predicate pushdown (filtering data at the source before reading it into memory) and columnar vectorization for significantly faster I/O and query processing. This is particularly advantageous compared to row-oriented formats or iterating through individual STAC items via an API for large-scale filtering.</li> <li> <strong>Organization</strong>: Typically, each set of GeoParquet files corresponds to STAC items from a specific data collection, maintaining a clear and organized data lake structure. The schema of the GeoParquet files is derived directly from the STAC Item structure, including common metadata fields, asset links, and the geometry.</li> </ul> <p><strong>2. DuckDB for Accelerated Indexing and Querying:</strong></p> <p>DuckDB, an in-memory OLAP (Online Analytical Processing) DBMS, is employed for its exceptional speed, ease of integration (especially its Python bindings and direct Parquet reading capabilities), and rich SQL dialect.</p> <p>Key DuckDB features utilized:</p> <ul> <li> <p>Direct Parquet Querying: DuckDB can directly query one or more Parquet files, including those stored in cloud object storage.</p> <p>Python</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>import duckdb
conn = duckdb.connect()
conn.sql("SELECT count(id) FROM 'path/to/your/collection_geoparquet/*.parquet';")
</code></pre></div> </div> </li> <li> <p>Spatial Extension: This extension is crucial for geospatial filtering. After installation (INSTALL spatial; LOAD spatial), powerful spatial SQL operations can be performed.</p> <p>Python</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code># Example Python usage with DuckDB
import duckdb
  
# It's good practice to install and load extensions once per session if needed.
# For persistent storage, these might be set in the database configuration.
conn = duckdb.connect()
conn.execute("INSTALL spatial;")
conn.execute("LOAD spatial;")
  
# Define WKT for an Area of Interest (AOI) and time range
aoi_wkt = "POLYGON((-5.0 47.0, -5.0 48.0, -4.0 48.0, -4.0 47.0, -5.0 47.0))" # Example polygon
start_time = "2023-03-01T00:00:00Z"
end_time = "2023-08-31T23:59:59Z"
  
query = f"""
SELECT id, properties_datetime, assets_B04_href, properties_eo_cloud_cover
FROM read_parquet('path_to_your_geoparquet_files/*.parquet', union_by_name=True)
WHERE "sar:product_type" = 'GRD' -- Example for Sentinel-1
  AND "sar:instrument_mode" = 'IW' -- Example for Sentinel-1
  AND ST_Intersects(geometry, ST_GeomFromText('{aoi_wkt}'))
  AND properties_datetime &gt;= '{start_time}'
  AND properties_datetime &lt;= '{end_time}'
  AND properties_eo_cloud_cover &lt; 20; -- Example cloud cover filter
"""
result = conn.execute(query).fetchdf()
print(result.head())
</code></pre></div> </div> </li> <li> <p>CQL2 to SQL Translation: To maintain compatibility with existing STAC API workflows and allow users to leverage familiar query languages, the pygeofilter library, along with its library, along with its pygeofilter-duckdb backend, can parse CQL2-JSON filters and translate them into DuckDB SQL WHERE clauses. This allows users to define filters once and apply them to both STAC APIs and the GeoParquet/DuckDB backend.</p> <p>Python</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code># Conceptual Python usage with pygeofilter
from pygeofilter.parsers.cql2_json import parse as json_parse
from pygeofilter.backends.duckdb import to_sql_where
from pygeofilter.util import IdempotentDict
  
cql2_filter = {
  "op": "and",
  "args": [
    {"op": "between", "args": [{"property": "eo:cloud_cover"}, 0, 21]},
    {"op": "between", "args": [{"property": "datetime"}, "2023-02-01T00:00:00Z", "2023-02-28T23:59:59Z"]},
    {"op": "s_intersects", "args": [{"property": "geometry"}, {"type": "Polygon", "coordinates": [[[...]]]}]}
  ]
}
# field_mapping can be used if property names differ from GeoParquet column names
sql_where_clause = to_sql_where(json_parse(cql2_filter), IdempotentDict())
# full_query = f"SELECT * FROM read_parquet('{geoparquet_path}') WHERE {sql_where_clause}"
</code></pre></div> </div> </li> </ul> <p>This spatio-temporal pre-filtering significantly prunes the search space, making the subsequent semantic retrieval step more focused and computationally feasible.</p> <h2 id="part-2-intelligent-collection-selection-via-multi-document-rag-with-llamaindex">Part 2: Intelligent Collection Selection via Multi-Document RAG with LlamaIndex</h2> <p>Even after spatio-temporal filtering, multiple data collections might meet the basic criteria. Selecting the <em>most suitable</em> collection for a nuanced task (e.g., “which LiDAR dataset is best for sub-meter vertical accuracy canopy height models in a temperate forest region?”) requires deeper semantic understanding of each collection’s specifications, processing levels, and suitability for specific applications. A Multi-Document Retrieval Augmented Generation (RAG) system, built using LlamaIndex, provides this capability.</p> <p><strong>1. Architecture Overview:</strong></p> <p>The system employs a two-tiered agent structure:</p> <ul> <li> <strong>Collection-Specific Agents (<code class="language-plaintext highlighter-rouge">FunctionAgent</code>)</strong>: Each candidate geospatial data collection, represented by its detailed metadata, user guides, scientific papers describing it, or even API documentation, is managed by a dedicated LlamaIndex <code class="language-plaintext highlighter-rouge">FunctionAgent</code>.</li> <li> <strong>Top-Level Orchestrator Agent (<code class="language-plaintext highlighter-rouge">FunctionAgent</code> or <code class="language-plaintext highlighter-rouge">ReActAgent</code>)</strong>: This agent receives the user’s primary query and intelligently routes sub-queries to the appropriate collection-specific agents.</li> </ul> <p><strong>2. Building Collection-Specific Agents:</strong></p> <p>The process, inspired by LlamaIndex’s multi-document agent patterns, involves creating specialized agents for each data collection:</p> <ul> <li> <p><strong>Document Processing</strong>: For each collection, relevant documents (e.g., landing pages, technical specifications, usage tutorials) are loaded and parsed into nodes using <code class="language-plaintext highlighter-rouge">SentenceSplitter</code>.</p> </li> <li> <p>Index Creation:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">VectorStoreIndex</code>: Built from these nodes using an embedding model (e.g., <code class="language-plaintext highlighter-rouge">OpenAIEmbedding(model="text-embedding-3-small")</code>). This index allows for semantic search within the collection’s documentation (e.g., “find information about radiometric correction”).</li> <li> <code class="language-plaintext highlighter-rouge">SummaryIndex</code>: Optionally, a <code class="language-plaintext highlighter-rouge">SummaryIndex</code> can be built to generate concise summaries of each document/collection. This is useful for providing quick overviews to the top-level agent or the user.</li> </ul> </li> <li> <p>Query Engines and Tools:</p> <ul> <li>The indices are exposed as query engines (e.g., <code class="language-plaintext highlighter-rouge">vector_index.as_query_engine()</code>, <code class="language-plaintext highlighter-rouge">summary_index.as_query_engine()</code>).</li> <li>These query engines are then wrapped into <code class="language-plaintext highlighter-rouge">QueryEngineTool</code> instances. Each tool is given a name and a description that outlines its specific capabilities (e.g., “Useful for answering specific factual questions about Collection X’s processing levels”).</li> </ul> </li> <li> <p><strong>Agent Definition</strong>: A <code class="language-plaintext highlighter-rouge">FunctionAgent</code> (often powered by an LLM like OpenAI’s <code class="language-plaintext highlighter-rouge">gpt-4o</code>) is instantiated for each collection. It’s equipped with the tools created above and a system prompt that directs it to use these tools to answer questions <em>exclusively</em> about its assigned data collection, avoiding reliance on prior knowledge. An asynchronous function <code class="language-plaintext highlighter-rouge">build_agent_per_doc</code> typically encapsulates this logic.</p> </li> </ul> <p><strong>3. Top-Level Orchestrator Agent:</strong></p> <ul> <li> <p><strong>Tool Aggregation</strong>: The specialized collection agents are exposed as tools to the top-level agent. This is done by wrapping the <code class="language-plaintext highlighter-rouge">agent.run</code> (or <code class="language-plaintext highlighter-rouge">agent.arun</code> for async) method of each collection agent into a <code class="language-plaintext highlighter-rouge">FunctionTool</code> using <code class="language-plaintext highlighter-rouge">FunctionTool.from_defaults</code>. The <code class="language-plaintext highlighter-rouge">description</code> of each <code class="language-plaintext highlighter-rouge">FunctionTool</code> is critical; it’s often derived from a summary of the collection the agent represents, enabling the top-level agent to make informed decisions about which tool (i.e., which collection agent) to engage for a given query.</p> </li> <li> <p>Tool Retrieval and Reranking :</p> <ol> <li>An <code class="language-plaintext highlighter-rouge">ObjectIndex.from_objects(all_tools, index_cls=VectorStoreIndex)</code> is created to index all the collection-agent tools.</li> <li>When the top-level agent receives a query, an initial set of relevant tools (collection agents) is retrieved using <code class="language-plaintext highlighter-rouge">obj_index.as_node_retriever(similarity_top_k=N)</code>.</li> <li>This retrieved set can be further refined using a postprocessor like <code class="language-plaintext highlighter-rouge">CohereRerank(top_n=M, model="rerank-english-v3.0")</code> (or similar) to improve the precision of tool selection.</li> <li>A <code class="language-plaintext highlighter-rouge">CustomObjectRetriever</code> (as demonstrated in LlamaIndex examples) can be implemented. This custom retriever can not only fetch the top N tools post-reranking but also dynamically inject a “comparison” or “query planning” sub-agent/tool. This sub-agent would take the original query and the selected tools as input, enabling it to explicitly compare information from multiple collections if the user’s query requires it (e.g., “Compare Sentinel-2 and Landsat 9 for vegetation monitoring”).</li> </ol> </li> <li> <p><strong>Query Execution</strong>: The top-level agent (e.g., a <code class="language-plaintext highlighter-rouge">FunctionAgent</code> or <code class="language-plaintext highlighter-rouge">ReActAgent</code> with an LLM like <code class="language-plaintext highlighter-rouge">gpt-4o</code>) then utilizes these selected and reranked tools (i.e., collection agents) to synthesize an answer to the user’s query. It effectively delegates sub-questions or information gathering tasks to the most relevant collection specialists.</p> </li> </ul> <p><strong>4. Output:</strong></p> <p>The system aims to identify the most suitable collection(s) for the user’s task. Furthermore, it can be prompted to extract key parameters (e.g., specific band names, asset keys for direct access, relevant processing levels, or even Python code snippets for data loading) based on the context retrieved by the RAG pipeline.</p> <p>Okay, I’ve revised the blog post to be more technical and included instructions for the flowchart.</p> <hr> <h2 id="the-first-step-of-open-geoagent-a-technical-dive-into-finding-useful-geospatial-data-sources">The First Step of Open GeoAgent: A Technical Dive into Finding Useful Geospatial Data Sources</h2> <p>Efficiently discovering and preparing optimal geospatial datasets from heterogeneous, large-scale open repositories like Google Earth Engine (GEE), Microsoft Planetary Computer, NASA Earth Data, and AWS Open Data is a critical bottleneck for advanced geospatial analysis and AI applications. Open GeoAgent’s initial phase tackles this by implementing a robust system for discovering and preparing relevant geospatial data sources.</p> <p>This post details a two-stage architecture:</p> <ol> <li>A <strong>data processing pipeline</strong> converting STAC (SpatioTemporal Asset Catalog) metadata to GeoParquet, indexed by DuckDB for high-performance spatio-temporal querying.</li> <li>An <strong>intelligent retrieval layer</strong> using a multi-document Retrieval Augmented Generation (RAG) system built with LlamaIndex to identify task-specific collections and extract necessary parameters.</li> </ol> <h3 id="part-1-high-performance-data-preparation-with-stac-geoparquet-and-duckdb-1">Part 1: High-Performance Data Preparation with STAC, GeoParquet, and DuckDB</h3> <p>The foundation of our data discovery pipeline is the standardized STAC metadata. To optimize for analytical queries, STAC Items from target collections across major open data providers are transformed into GeoParquet files.</p> <p><strong>1. STAC to GeoParquet Transformation:</strong></p> <ul> <li> <strong>Why GeoParquet?</strong> GeoParquet is an open, cloud-optimized, columnar format for geospatial vector data. Its columnar nature is key, allowing efficient storage and partial reads. This structure enables query engines like DuckDB to leverage predicate pushdown (filtering data at the source before reading it into memory) and columnar vectorization for significantly faster I/O and query processing. This is particularly advantageous compared to row-oriented formats or iterating through individual STAC items via an API for large-scale filtering.</li> <li> <strong>Organization</strong>: Typically, each set of GeoParquet files corresponds to STAC items from a specific data collection, maintaining a clear and organized data lake structure. The schema of the GeoParquet files is derived directly from the STAC Item structure, including common metadata fields, asset links, and the geometry.</li> </ul> <p><strong>2. DuckDB for Accelerated Indexing and Querying:</strong></p> <p>DuckDB, an in-memory OLAP (Online Analytical Processing) DBMS, is employed for its exceptional speed, ease of integration (especially its Python bindings and direct Parquet reading capabilities), and rich SQL dialect.</p> <p>Key DuckDB features utilized:</p> <ul> <li> <p>Direct Parquet Querying: DuckDB can directly query one or more Parquet files, including those stored in cloud object storage.</p> <p>Python</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>import duckdb
conn = duckdb.connect()
conn.sql("SELECT count(id) FROM 'path/to/your/collection_geoparquet/*.parquet';")
</code></pre></div> </div> </li> <li> <p>Spatial Extension: This extension is crucial for geospatial filtering. After installation (INSTALL spatial; LOAD spatial;), powerful spatial SQL operations can be performed.</p> <p>Python</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code># Example Python usage with DuckDB
import duckdb
  
# It's good practice to install and load extensions once per session if needed.
# For persistent storage, these might be set in the database configuration.
conn = duckdb.connect()
conn.execute("INSTALL spatial;")
conn.execute("LOAD spatial;")
  
# Define WKT for an Area of Interest (AOI) and time range
aoi_wkt = "POLYGON((-5.0 47.0, -5.0 48.0, -4.0 48.0, -4.0 47.0, -5.0 47.0))" # Example polygon
start_time = "2023-03-01T00:00:00Z"
end_time = "2023-08-31T23:59:59Z"
  
query = f"""
SELECT id, properties_datetime, assets_B04_href, properties_eo_cloud_cover
FROM read_parquet('path_to_your_geoparquet_files/*.parquet', union_by_name=True)
WHERE "sar:product_type" = 'GRD' -- Example for Sentinel-1
  AND "sar:instrument_mode" = 'IW' -- Example for Sentinel-1
  AND ST_Intersects(geometry, ST_GeomFromText('{aoi_wkt}'))
  AND properties_datetime &gt;= '{start_time}'
  AND properties_datetime &lt;= '{end_time}'
  AND properties_eo_cloud_cover &lt; 20; -- Example cloud cover filter
"""
result = conn.execute(query).fetchdf()
print(result.head())
</code></pre></div> </div> </li> <li> <p>CQL2 to SQL Translation: To maintain compatibility with existing STAC API workflows and allow users to leverage familiar query languages, the</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>pygeofilter
</code></pre></div> </div> <p>library, along with its</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>pygeofilter-duckdb
</code></pre></div> </div> <p>backend, can parse CQL2-JSON filters and translate them into DuckDB SQL</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>WHERE
</code></pre></div> </div> <p>clauses. This allows users to define filters once and apply them to both STAC APIs and the GeoParquet/DuckDB backend.</p> <p>Python</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code># Conceptual Python usage with pygeofilter
from pygeofilter.parsers.cql2_json import parse as json_parse
from pygeofilter.backends.duckdb import to_sql_where
from pygeofilter.util import IdempotentDict
  
cql2_filter = {
  "op": "and",
  "args": [
    {"op": "between", "args": [{"property": "eo:cloud_cover"}, 0, 21]},
    {"op": "between", "args": [{"property": "datetime"}, "2023-02-01T00:00:00Z", "2023-02-28T23:59:59Z"]},
    {"op": "s_intersects", "args": [{"property": "geometry"}, {"type": "Polygon", "coordinates": [[[...]]]}]}
  ]
}
# field_mapping can be used if property names differ from GeoParquet column names
sql_where_clause = to_sql_where(json_parse(cql2_filter), IdempotentDict())
# full_query = f"SELECT * FROM read_parquet('{geoparquet_path}') WHERE {sql_where_clause}"
</code></pre></div> </div> </li> <li> <dl> <dt>STAC Item Reconstruction</dt> <dd> <p>Query results from DuckDB (often retrieved as Arrow tables or Pandas DataFrames) can be efficiently converted back to</p> </dd> </dl> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>pystac.Item
</code></pre></div> </div> <p>objects using the</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>stac_geoparquet.arrow._api.stac_table_to_items
</code></pre></div> </div> <p>function. This ensures seamless integration with downstream STAC-aware tools and libraries.</p> <p>Python</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code># Conceptual Python snippet (based on your provided material)
# import json
# import pystac
# from stac_geoparquet.arrow._api import stac_table_to_items
# db_result_arrow_table = conn.execute(full_query).fetch_arrow_table()
# stac_items = []
# for item_dict in stac_table_to_items(db_result_arrow_table):
#     item_dict['assets'] = json.loads(item_dict['assets']) # Assets might be stored as JSON strings
#     stac_items.append(pystac.Item.from_dict(item_dict))
</code></pre></div> </div> </li> </ul> <p>This spatio-temporal pre-filtering significantly prunes the search space, making the subsequent semantic retrieval step more focused and computationally feasible.</p> <h3 id="part-2-intelligent-collection-selection-via-multi-document-rag-with-llamaindex-1">Part 2: Intelligent Collection Selection via Multi-Document RAG with LlamaIndex</h3> <p>Even after spatio-temporal filtering, multiple data collections might meet the basic criteria. Selecting the <em>most suitable</em> collection for a nuanced task (e.g., “which LiDAR dataset is best for sub-meter vertical accuracy canopy height models in a temperate forest region?”) requires deeper semantic understanding of each collection’s specifications, processing levels, and suitability for specific applications. A Multi-Document Retrieval Augmented Generation (RAG) system, built using LlamaIndex, provides this capability.</p> <p><strong>1. Architecture Overview:</strong></p> <p>The system employs a two-tiered agent structure:</p> <ul> <li> <strong>Collection-Specific Agents (<code class="language-plaintext highlighter-rouge">FunctionAgent</code>)</strong>: Each candidate geospatial data collection, represented by its detailed metadata, user guides, scientific papers describing it, or even API documentation, is managed by a dedicated LlamaIndex <code class="language-plaintext highlighter-rouge">FunctionAgent</code>.</li> <li> <strong>Top-Level Orchestrator Agent (<code class="language-plaintext highlighter-rouge">FunctionAgent</code> or <code class="language-plaintext highlighter-rouge">ReActAgent</code>)</strong>: This agent receives the user’s primary query and intelligently routes sub-queries to the appropriate collection-specific agents.</li> </ul> <p><strong>2. Building Collection-Specific Agents:</strong></p> <p>The process, inspired by LlamaIndex’s multi-document agent patterns, involves creating specialized agents for each data collection:</p> <ul> <li> <p><strong>Document Processing</strong>: For each collection, relevant documents (e.g., landing pages, technical specifications, usage tutorials) are loaded and parsed into nodes using <code class="language-plaintext highlighter-rouge">SentenceSplitter</code>.</p> </li> <li> <p>Index Creation</p> <p>:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">VectorStoreIndex</code>: Built from these nodes using an embedding model (e.g., <code class="language-plaintext highlighter-rouge">OpenAIEmbedding(model="text-embedding-3-small")</code>). This index allows for semantic search within the collection’s documentation (e.g., “find information about radiometric correction”).</li> <li> <code class="language-plaintext highlighter-rouge">SummaryIndex</code>: Optionally, a <code class="language-plaintext highlighter-rouge">SummaryIndex</code> can be built to generate concise summaries of each document/collection. This is useful for providing quick overviews to the top-level agent or the user.</li> </ul> </li> <li> <p>Query Engines and Tools</p> <p>:</p> <ul> <li>The indices are exposed as query engines (e.g., <code class="language-plaintext highlighter-rouge">vector_index.as_query_engine()</code>, <code class="language-plaintext highlighter-rouge">summary_index.as_query_engine()</code>).</li> <li>These query engines are then wrapped into <code class="language-plaintext highlighter-rouge">QueryEngineTool</code> instances. Each tool is given a name and a description that outlines its specific capabilities (e.g., “Useful for answering specific factual questions about Collection X’s processing levels”).</li> </ul> </li> <li> <p><strong>Agent Definition</strong>: A <code class="language-plaintext highlighter-rouge">FunctionAgent</code> (often powered by an LLM like OpenAI’s <code class="language-plaintext highlighter-rouge">gpt-4o</code>) is instantiated for each collection. It’s equipped with the tools created above and a system prompt that directs it to use these tools to answer questions <em>exclusively</em> about its assigned data collection, avoiding reliance on prior knowledge. An asynchronous function <code class="language-plaintext highlighter-rouge">build_agent_per_doc</code> typically encapsulates this logic.</p> </li> </ul> <p><strong>3. Top-Level Orchestrator Agent:</strong></p> <ul> <li> <p><strong>Tool Aggregation</strong>: The specialized collection agents are exposed as tools to the top-level agent. This is done by wrapping the <code class="language-plaintext highlighter-rouge">agent.run</code> (or <code class="language-plaintext highlighter-rouge">agent.arun</code> for async) method of each collection agent into a <code class="language-plaintext highlighter-rouge">FunctionTool</code> using <code class="language-plaintext highlighter-rouge">FunctionTool.from_defaults</code>. The <code class="language-plaintext highlighter-rouge">description</code> of each <code class="language-plaintext highlighter-rouge">FunctionTool</code> is critical; it’s often derived from a summary of the collection the agent represents, enabling the top-level agent to make informed decisions about which tool (i.e., which collection agent) to engage for a given query.</p> </li> <li> <p>Tool Retrieval and Reranking :</p> <ol> <li>An <code class="language-plaintext highlighter-rouge">ObjectIndex.from_objects(all_tools, index_cls=VectorStoreIndex)</code> is created to index all the collection-agent tools.</li> <li>When the top-level agent receives a query, an initial set of relevant tools (collection agents) is retrieved using <code class="language-plaintext highlighter-rouge">obj_index.as_node_retriever(similarity_top_k=N)</code>.</li> <li>This retrieved set can be further refined using a postprocessor like <code class="language-plaintext highlighter-rouge">CohereRerank(top_n=M, model="rerank-english-v3.0")</code> (or similar) to improve the precision of tool selection.</li> <li>A <code class="language-plaintext highlighter-rouge">CustomObjectRetriever</code> (as demonstrated in LlamaIndex examples) can be implemented. This custom retriever can not only fetch the top N tools post-reranking but also dynamically inject a “comparison” or “query planning” sub-agent/tool. This sub-agent would take the original query and the selected tools as input, enabling it to explicitly compare information from multiple collections if the user’s query requires it (e.g., “Compare Sentinel-2 and Landsat 9 for vegetation monitoring”).</li> </ol> </li> <li> <p><strong>Query Execution</strong>: The top-level agent (e.g., a <code class="language-plaintext highlighter-rouge">FunctionAgent</code> or <code class="language-plaintext highlighter-rouge">ReActAgent</code> with an LLM like <code class="language-plaintext highlighter-rouge">gpt-4o</code>) then utilizes these selected and reranked tools (i.e., collection agents) to synthesize an answer to the user’s query. It effectively delegates sub-questions or information gathering tasks to the most relevant collection specialists.</p> </li> </ul> <p><strong>4. Output:</strong></p> <p>The system aims to identify the most suitable collection(s) for the user’s task. Furthermore, it can be prompted to extract key parameters (e.g., specific band names, asset keys for direct access, relevant processing levels, or even Python code snippets for data loading) based on the context retrieved by the RAG pipeline.</p> <h3 id="conclusion-and-future-work">Conclusion and Future Work</h3> <p>This two-stage architecture – combining high-performance pre-filtering with DuckDB/GeoParquet and sophisticated semantic retrieval with LlamaIndex RAG – forms a robust and technically sound first step for Open GeoAgent. It addresses the dual challenges of massive data volume and the need for nuanced semantic relevance in data discovery, paving the way for more automated, intelligent, and insightful geospatial data utilization. Future enhancements will focus on loading data from queried data collection to local data analysis.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/displaying-external-posts-on-your-al-folio-blog/">Displaying External Posts on Your al-folio Blog</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/VQA/">A Preliminary Study of Visual Prompt-Based Open-Vocabulary RSI Segmentation and Change Detection in VLMs</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/diagrams/">a post with diagrams</a> </li> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2025 Yusin Chen. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>